{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 9 - 26.5.19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing and Test-Driven Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've already met one widely adopted programming technique which isn't used as much in the academy - object-oriented programming. Another such technique, ubiquitously used wherever code is written _outside_ the academy, is testing. In my opinion, this is a crucial topic in software development that isn't treated with the respect it should in the academy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests are (usually) short pieces of code designed to assert that a small portion of your program does what you intend it to do. \n",
    "\n",
    "For example, if I have a class designed to perform calculations on some DataFrame that was created somewhere else, perhaps by adding some of its columns together, averaging them out and displaying the result, then I wish for my code to be correct and deterministic, in a sense that a single, defined input will always give the same correct output.\n",
    "\n",
    "This isn't trivial even for the most basic functions in Python. Let's try to build a \"wrapper\" over `np.tile`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 1, 2, 3, 1, 2, 3],\n",
       "       [1, 2, 3, 1, 2, 3, 1, 2, 3]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A reminer of np.tile() - MATLAB's repmat\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "arr = np.array([1, 2, 3])\n",
    "tiled = np.tile(arr, (2, 3))  # repeat it twice in the row dimension (axis=0), \n",
    "                              # and three times in the column dimension\n",
    "tiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our own implementation\n",
    "\n",
    "class Tiler:\n",
    "    \"\"\" Tile any number of objects on their first axis \"\"\"\n",
    "    def __init__(self, *args):\n",
    "        self.to_tile = args\n",
    "\n",
    "    def tile(self, reps=(1,)):\n",
    "        self.tiled = np.tile(self.to_tile, reps=reps)\n",
    "        return self.tiled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above class can tile as many array inputs as you wish, and currently it's just a fancy wrapper over `np.tile()`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "------\n",
      "[[1 2 3 1 2 3]\n",
      " [4 5 6 4 5 6]\n",
      " [1 2 3 1 2 3]\n",
      " [4 5 6 4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "tiled = Tiler([1, 2, 3], [4, 5, 6]).tile()\n",
    "print(tiled)\n",
    "\n",
    "print('------')\n",
    "tiled_again = Tiler([1, 2, 3], [4, 5, 6]).tile((2, 2))\n",
    "print(tiled_again)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a very simple implementation, with the only \"new\" thing being the `*args` argument to the `__init__` function. `*args` means: Pack into a tuple called `args` all non-keyword arguments that the function receives. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a is 1\n",
      "b is 2\n",
      "The rest of the data received is (3, 4, 'a')\n"
     ]
    }
   ],
   "source": [
    "def f(a, b, *args):\n",
    "    print(f\"a is {a}\")\n",
    "    print(f\"b is {b}\")\n",
    "    print(f\"The rest of the data received is {args}\")\n",
    "    \n",
    "f(1, 2, 3, 4, 'a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `*args` when we wish to write a function with a non-constant number of arguments. The \"magic\" is done with the star (`*`), the `args` is just a convention, it's not a keyword. The star knows to bind the inputs into a tuple. In our case, we wish the `Tiler` class to tile as many arrays as the user wishes. This is most easily done with the `*args` parameter. Like `*args`, Python also has `**kwargs` that binds the keyword argument inputs into a dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now that we understand the class completely - does it work?\n",
    "\n",
    "It does work, but it's not _guaranteed_ to work. Some issues are clear and can be solved immediately, like some basic type-checking that we should implement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tiler:\n",
    "    \"\"\" \n",
    "    Tile any number of objects on their first axis.\n",
    "    Returns: np.ndarray tiled across the first dimension.\n",
    "    Errors: TypeError if supplied with invalid iterables\n",
    "    \"\"\"\n",
    "    def __init__(self, *args):\n",
    "        _to_tile = self._verify_inp(args)\n",
    "        self.to_tile = _to_tile\n",
    "\n",
    "    def tile(self, reps=(1,)):\n",
    "        self.tiled = np.tile(self.to_tile, reps=reps)\n",
    "        return self.tiled\n",
    "    \n",
    "    def _verify_inp(self, it):\n",
    "        to_tile = []\n",
    "        for item in it:\n",
    "            if not isinstance(item, (list, np.ndarray)):\n",
    "                raise TypeError(f'Item {item} not a valid iterable to tile.')\n",
    "            to_tile.append(item)\n",
    "        return to_tile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the class specifies its return type and what happens in case of an error. A good application will know to wrap the use of the class in a `try... except` block and catch any `TypeError`s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "--------\n",
      "TypeError: Item {4, 5, 6} not a valid iterable to tile.\n"
     ]
    }
   ],
   "source": [
    "# This works:\n",
    "try:\n",
    "    tiled = Tiler([1, 2, 3], [4, 5, 6]).tile()\n",
    "except TypeError as e:\n",
    "    print(f\"TypeError: {e}\")\n",
    "print(tiled)\n",
    "\n",
    "# This shouldn't\n",
    "print(\"--------\")\n",
    "try:\n",
    "    tiled_wrong = Tiler([1, 2, 3], {4, 5, 6}).tile()  # input is a set, not a list\n",
    "except TypeError as e:\n",
    "    print(f\"TypeError: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are we covered? Is this class no longer vulnerable to any type of non-legitimate input? Of course it is vulnerable, and some of us might already see a few corner cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([]), array([1, 2, 3])], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    tiled = Tiler([], np.array([1, 2, 3])).tile()\n",
    "except TypeError as e:\n",
    "    print(f\"TypeError: {e}\")\n",
    "\n",
    "tiled  # what is that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This array of lists and arrays is certainly not what we had in mind when tiling an empty list an a simple array. We can think what exactly we expect this function to do, but this output will certainly not be on the list of possible outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I really don't have to convince you that software, even the most basic application, has bugs. But _thinking_ that we solved our bugs by the insertion of methods like `self._verify_inp` doesn't mean we actually solved them.\n",
    "\n",
    "To this end we write tests for our program. You've already seen unit tests in your homework assignments. You were asked to make sure that your solution passes all tests before submitting. This exemplifies a way through which we can be more certain that our program does what we thought it was doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests are important to us for two reasons. The first one is that even simple programs are more complicated than we think. The `Tiler` class is a good example, but you might imagine that as soon as we add interfaces between classes, methods and functions, things might get a bit messy. For example, in the aviation industry, for each line of code a software has, you may find about 8 lines dedicated to testing it.\n",
    "\n",
    "Moreover, when we deal with user input - data files, parameters for script - we should expect the unexpected, even if the main user is ourselves. Our future self in a few months will probably not remember the type of every parameter it has to enter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second reason is Python's dynamic nature, or _duck-typing._ If you want to enforce a function to only accept inputs of a single type, _you_ must be the one writing these assertions, either outside or inside the function. For example, a function that adds two numbers needs a `isinstance(value, (int, float))` somewhere near its top to avoid these mistakes. Statically-typed languages, like C, define a type for each variable. A function adding two integers simply cannot accept a non-integer input. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python's dynamic nature is a blessing on many occasions, but it can sometimes be a real pain. This nature is the second important reason to write tests to our code. Many cases that in other programming languages would've resulted in a simple `TypeError`, can cause major bugs in Python due to wrong input types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test-Driven Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test-Driven Development, or TDD, is a very popular way to solve the problems we described. In TDD we reverse the process of writing code. We first write a test to the function we wish to write. It fails - because the function doesn't exist yet - and then we write then function until we're passing our test. We then add more tests to make sure that the function works properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume I wish to write a function that adds two positive integers. First I'll write a few basic tests for the function, and then I'll try to run them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test functions always start with \"test_...\"\n",
    "def test_basic_addition():\n",
    "    first = 2\n",
    "    second = 4\n",
    "    result = 6\n",
    "    return intadd(first, second) == result\n",
    "    \n",
    "def test_negative_inp1():\n",
    "    try:\n",
    "        result = intadd(-1, 1)\n",
    "    except TypeError:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def test_negative_inp2():\n",
    "    try:\n",
    "        result = intadd(1, -1)\n",
    "    except TypeError:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the tests are written (and are failing, since the fucntion `intadd` doesn't exist, we need to write the function so that it will comply with the tests we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intadd(num1, num2):\n",
    "    \"\"\" Non-negative integer addition \"\"\"\n",
    "    if (num1 < 0) or (num2 < 0):\n",
    "        raise TypeError('Input must be positive.')\n",
    "    return num1 + num2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(test_basic_addition())\n",
    "print(test_negative_inp1())\n",
    "print(test_negative_inp2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our function works! See how each test deals with a unique sub-case of failure? It's much easier to debug and answer \"what went wrong?\" when you tests are so precise. A good test consists of a couple of lines of initialization, a couple of lines that call the tested function, and the `assert`ing line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the time it took me to write the implementation, I thought of a different edge case - floating point numbers input. Now I should write tests that will check this input type, they will fail - and then I can refactor my `intadd` function to deal with these cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_float_inp():\n",
    "    \"\"\" A shorter version - check both inputs in a single test function \"\"\"\n",
    "    try:\n",
    "        result = intadd(1., 2)\n",
    "    except TypeError:\n",
    "        try:\n",
    "            result = intadd(2, 1.)\n",
    "        except TypeError:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(test_float_inp())  # False!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intadd(num1, num2):\n",
    "    \"\"\" Non-negative integer addition \"\"\"\n",
    "    if (num1 < 0) or (num2 < 0):\n",
    "        raise TypeError('Input must be positive.')\n",
    "    if isinstance(num1, float) or isinstance(num2, float):\n",
    "        raise TypeError('Input must be integer.')\n",
    "    return num1 + num2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(test_float_inp())  # True!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(test_basic_addition())\n",
    "print(test_negative_inp1())\n",
    "print(test_negative_inp2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the process continues - every time I think of more edge cases, I write failing tests and then refactor my own function.\n",
    "\n",
    "Another important thing to note here is the way TDD forces me to \"design the API\" of my function. As you can see, when I wrote the test I had to think of the output I'll receive when I enter wrong inputs. In this case I thought it was most appropriate for the function to return an TypeError - an exception that if not caught can terminate the execution of my application. This appeared to be the right choice here, but sometimes we can define a different result for faulty inputs. We could've returned `-1` if the input was invalid, for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With TDD, most people don't actually write the first tests before they write the function. TDD purists might insist on that, but for all intents and purposes, if you keep the development of the tests very closely tied to the development of the function itself - you're doing TDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual usage - `pytest`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might have guessed, the Python ecosystem offers tools to automate the testing process. The standard library comes with a `unittest` module, and another famous one is `nosetests`. But the most popular (and advanced) library is [`pytest`](https://docs.pytest.org/en/latest/), and that will be the our library of choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `tests_demo` folder you can see how one structures the tests a project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating this file structure, you just call `pytest` in the command line after you `cd`ed into the folder containing the project - and all tests are run for you.\n",
    "\n",
    "`pytest` has some advanced features. In the tests inside `tests_demo` you could already see the `parametrize` decorator, used to call the same test with several different inputs. `pytest` is smart enough to tell us which input out of the ones we entered caused the exception. In addition, you can also mark some tests as \"expected to fail\". Finally, It can also create automatic tests for you. [Here](https://docs.pytest.org/en/latest/parametrize.html#basic-pytest-generate-tests-example) you'll find the formal docs, and [here](https://hackebrot.github.io/pytest-tricks/create_tests_via_parametrization/) you can find a clear blog post explaining it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A few extra points:\n",
    "\n",
    "1. Tests should be as concise as possible. Their execution time should be minimal.\n",
    "2. Run your test suit as often as possible. Minimal frequency is before each commit you make.\n",
    "3. You should try hard to translate bugs into tests. These might be the most important tests you'll write.\n",
    "4. Test names are long. It's fine - it's because we don't use docstrings for tests.\n",
    "5. VSCode can be set up to look for `pytest` in your current environment, and run your tests at certain points in time. To do so, go to _File -> Preferences -> Settings_, search for \"test\" and check \"Enable unit testing using pytest\", and make sure that both \"Enable unit testing using nosetests\" and \"Enable unit testing using unittest\" are unchecked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integration Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unit tests repeatedly tests functions or methods your write under different inputs, and are the backbone of any reliable test suit. However, unit tests are not enough, since they don't check the interface between the different functions and classes in your application.\n",
    "\n",
    "Integration tests are larger, heavier tests that take at least two components, or units, of your application, and makes sure that they interact well with each other.\n",
    "\n",
    "Obviously, if we start taking each two consecutive functions and write an integration test for that pair, and then continue with all three consecutive functions and write these integration tests, and so on - we'll never finish writing the damn application. That's why integration testing is used at crucial junctions of our application, between major classes for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a class that returns the difference between the Fibonacci series and the prime numbers series, up to some _n._ The output should be an array of numbers, n-items long, and a plot to accompany it. As an example, for $n=3$, I expect the array to be `np.array([-2, -2, -4])`. Write the class in a test-driven development style.\n",
    "\n",
    "A reminder, the Fibonacci series is a series of numbers starting from (0, 1), with its next element being the sum of the previous two numbers: 0, 1, 1, 2, 3, 5, ... Prime numbers start from 2 and are only divisible by themselves and 1 without a remainder.\n",
    "\n",
    "You decide on the class' interface and the details of implementation. You may use `numpy`, and I insist that you write at least 5 unit tests and 1 integration test for this class.\n",
    "\n",
    "Don't try to implement things in a performant way, with fancy algorithms. The focus here is the unit tests and test-driven development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise solutions are in the directory `fib_ser`"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
